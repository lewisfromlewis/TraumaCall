---
title: "Mitch"
author: "Lewis"
date: "13 July 2017"
output: github_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
```

***Trauma Call in Darwin***
*Team Activation Criteria as a predictor of severe trauma; an analysis of the sensitivity and specificity of Royal Darwin Hospital's Trauma Team Callout Criteria.*

**Background**
Studies have shown that patients who have suffered severe trauma have improved outcomes when a multidisciplinary team and attendant resources ("Trauma Team") are mobilised from other tasks on patient arrival to the Emergency Department (ED) 1,2.  In seeking to maximise the benefit of hospital behaviour at the population level, the benefit to severely injured trauma patients is balanced against the effect of removing resources from other areas.  This balance is unlikely to be the same in every trauma centre.  For example, in a hospital with a higher proportion of severely injured patients who arrive in the early phase of trauma, plus both a low *rate* and a constant and low illness *severity* of patients presenting with non traumatic illness, the prior probability of severe trauma that is considered by the person performing triage is high, and the impact of Trauma Team callouts on service for other patients is low, easily predicted and easily mitigated.  This logic underlies the recommendations of many mature health systems for a single trauma centre covering a primary population of 2.5 million (refs?).

Trauma team composition is not closely specified at a national or international level and therefore differs between institutions.  Criteria for trauma team activation also differ mildly between institutions (refs?), as do the characteristics of the patients who arrive at the hospital with trauma and are screened for trauma team activation. The literature also presents a divergent set of clinical and administrative outcomes. The rules by which the Trauma Team is activated have been evaluated by determining their accuracy in the diagnosis of "Severe Trauma" (table / box). Some studies find that mechanism of injury criteria have low accuracy in predicting severe injury 3-5.

Our institution, Royal Darwin Hospital (RDH), is a remote hospital covering a large geographical area with a small population (ref).  There is a high incidence of severe trauma but long retrieval times are common
# (ref from Kath's work; if influential we could add a column of "injury time" and do a quick dirty analysis of that)
and there is a high population burden of non traumatic illness which is both seasonally varying and unpredictably varying (ref).  A dedicated Trauma Service reviews presentations twice daily to identify patients who may have trauma as a reason for admission, maintains a register of current and previous trauma patients and coordinates ongoing care of trauma patients using a nurse led model with close consultant input and 24-hour access to a senior surgical trainee designated as a Trauma Fellow.  Trauma patients admitted to ICU are additionally recorded on a binational registry of all ICU patients. 

The "Trauma Team Callout Criteria" (TTCC) at Royal Darwin Hospital (RDH) is a list of conditions based on pre hospital mechanism of injury and clinical observations on arrival at hospital.  If any one or more of the TTCC are present then a response is activated which is single step and essentially hospital wide (fig1).  A second, lower level of response is initiated on meeting less stringent criteria (fig2).  Our aim, using standard definitions from the literature, was to evaluate the performance of the TTCC in predicting Severe Trauma in the unique cohort of patients whom we serve.

```{r}
library(tidyverse)
library(lubridate)
library(readxl)
library(car)
library(lmtest)
## Next line is run once then replaced with the .csv file in the repo,as the subsequent line: 
## traumata <- read_xlsx("TraumadataV48.xlsx", sheet = 1, n_max = 784)
traumata <- read_csv("traumata.csv")
```
***Methods***
**Population**
A list of all patients presenting to RDH ED in the calendar year of 2015 formed the sample frame.  By linkage with the Trauma Registry, ICU Registry, administrative data on mortality and Operating Theatre records we identified 784 patients who had apparently been screened for severe trauma, or experienced a severe trauma event, out of XX total patients presenting to the Emergency Department.  One decision we faced in this evaluation was the choice of the denominator population.  The diagnostic contingency table (table) will generate higher values for negative predictive value and specificity as the proportion of patients screened in addition to those with severe trauma.

***Results***
**Population**
The severity of the ICU cohort for that year is indicated by the median Risk of Death based on the ANZICS APACHEIII score is given below.  The various risk functions give varying risks of death as summarised below, and the standardised mortality ratio across the year is from 0.42 to 0.53 (0.53 using APACHEIII).

```{r}
## Set the scene.
## Next line is run once then replaced with the .csv file in the repo
## Backgroundrate <- read_excel("./Cameron M/DeidentNonop2015.xlsx", sheet = 1)
Backgroundrate <- read_csv('Backgroundrate.csv')
deathtable <- table(Backgroundrate$ICUVitalStatus)
deathrate <- (deathtable[2]/deathtable[1])
deathpredictions <- Backgroundrate[,c(31, 33, 37)]
summary(deathpredictions)
predicteddeaths <- sapply(deathpredictions, sum, na.rm=T)
deathprobabilities <- sapply(deathpredictions, sum, na.rm=T)/dim(deathpredictions)[1]
SMR <- deathrate/deathprobabilities
print(SMR)
```
Major trauma occurred in 31.5% of those with trauma call criteria, but still seen in 8.1% of those without trauma call criteria.  This is a very influential odds ratio of 5.20 (p value 2.2E-16).
```{r}
table(traumata$`Meets TCC`, traumata$`Major Trauma`)
TCCpredictsmajor <- glm(traumata$`Major Trauma`=='yes' ~ traumata$`Meets TCC`, family="binomial")
anova(TCCpredictsmajor, test="Chisq")
```

Trauma call criteria were more often seen in those who died, odds ratio 4.09, p=0.004 by Chi squared.  For information, the background odds of death for those without a trauma call was 0.01 hence odds of death in those with a trauma call were still only 0.05.
```{r}
table(traumata$`Meets TCC`, traumata$`Discharge Status`)
TCCpredictsdeath <- glm(traumata$`Discharge Status`=='Dead' ~ traumata$`Meets TCC`, family = "binomial")
anova(TCCpredictsdeath, test="Chisq")
```
**Contribution of Criteria**
Examining the informational contribution of trauma call criteria using multivariable logistic regression.  Each component of the trauma call criteria is added sequentially to a multivariable model unless the data are sparse for one of the predictors: because the chi squared test is to be used this means any comparison with a cell count less than 5.  Logistic regression is one of the family of general linear models, which use the value of parameters to predict a response, assuming that the relationship across the values of the parameter holds true.  In each linear model there's a link function, so for a simple linear model it's the identity function, for logistic regression it's the logit function; and there's an assumption about an error structure for the data, in this case I've chosen binomial. So the model produces a set of coefficients, which in this case are the **log odds ratio** for major trauma with each of the predictors, "all else being equal".  So the odds ratio is their exponent to the base e.


```{r}
## Quick glance at the cell size for comparisons:
sparsity <- sapply(traumata[,22:62], table)
sparsity
## Excluded by this method (all but Neuro Def had cell counts 5 or fewer; neuro def was 6 but collinear with motor def): CPR + `Motor Loss` + `Sens Loss` + `Neuro Def` + Cyanosis + `RR<8` + `Amput Limb` + `CR<2s` + `HR<50` + `Inhal Burns` + `Blast Injury` + Drowning + `Crush head` + `Crush neck` + `Pen head` + `Bicycle vs car`
TCClist <- as.data.frame(traumata[,c(8,9, 22:30, 32, 33, 35:37, 39:62)])
wholemodel <- glm(formula = `Major Trauma`=='yes' ~ .,
                  family = "binomial", data = TCClist)
# I can't bring myself to delete this after typing the fucker in: wholemodel <- glm(formula = `Major Trauma`=='yes' ~ `multi?` + `MVA ejection` + `MVA entrapment` + `MVA fatality at scene` + `Ped vs car` + `Pen neck` + `Pen torso` + `Crush torso` + `Burns >15%` + `Near Drown` + `Fall >3m` + `Air Comp` + Intubated + `Sev fac inj` + `HR>120` + `SBP<90` + `Pel Unstab` + `RR>30` + `SaO2<90` + `Resp distr` + Flail + `GCS ≤13` + Agitated + Seizure + `Sig Inj ≥2` , family = "binomial", data = traumata)
wholemodel
```
Some model diagnostic, as follow, show this is a slightly flaky model.  Have I missed any of the trauma call criteria?  There are a lot even just in this list.  Drop1 reanalyses the model after dropping each one in order. Analysis of variance is the general term for analysing the contribution of each predictor to the total variability in the response.

The inverse model is the one excluding those predictors with most evidence for association based on the F test.  Finally, the reduced dataset of only complete cases has to be used for the likelihood ratio test of goodness of model fit, because of the assumption that one model is "nested" in the other.  Doing all this, which is quite standard, the inverse model performs almost exactly as well as the full bhoona.  This suggests that most of the trauma call criteria are correlated with one another and that at least most of them are redundant, given the rest.  At the bottom are the most tedious of all, which I really need a willing slave to do: the one by one correlations of each commonly occurring criterion with the others, stratified by Major Trauma definitions.
```{r}
drop1(wholemodel, test="LR")
Anova(wholemodel, test="F")
vif(wholemodel)

inversemodel <- update(wholemodel, drop.terms(wholemodel$terms, c(3, 10, 12, 13, 18, 19, 21, 22, 25)))
#Generate the list of complete cases among the trauma call criteria, redo both models
traumacompletecrit <- TCClist[complete.cases(TCClist),]
wholemodelcomplete <- glm(formula = `Major Trauma`=='yes' ~ .,
                  family = "binomial", data = traumacompletecrit)
inversemodelcomplete <- update(wholemodelcomplete, drop.terms(wholemodelcomplete$terms, c(3, 10, 12, 13, 18, 19, 21, 22, 25)))
lrtest(wholemodelcomplete, inversemodelcomplete)

mantelhaen.test(traumata$`Major Trauma`, traumata$`multi?`, traumata$`SaO2<90`)
mantelhaen.test(traumata$`Major Trauma`, traumata$Flail, traumata$`SaO2<90`)
mantelhaen.test(traumata$`Major Trauma`, traumata$`GCS ≤13`, traumata$`SaO2<90`)
mantelhaen.test(traumata$`Major Trauma`, traumata$Intubated, traumata$`SaO2<90`)
mantelhaen.test(traumata$`Major Trauma`, traumata$`multi?`, traumata$`SaO2<90`)

```

The next models are tedious and painful but give some idea of the predictive contribution of parameters alone.  In the first, only the commonest items are kept and result in a likelihood little lower than with the verbose model *in this sample*.  That may not be the case elsewhere, of course; but it is the case with the data we have.  The most important terms in *this* model, with log odds ratios of over 20, are a significant injury to more than one area, having been intubated, having a "flail chest" whatever that's worth and near drowning. In the presence of these, the other odds are less impressive.  Look below at bulkminimisedmodel to see how this is perhaps not as trustworthy as it seems, then check the coefficients of inversemodel, in which all of the significant predictors from wholemodel have been removed.  Agitation, for example, had OR e^(0.32)=1.38, when it's doing more of the heavy lifting that was previously taken by hypoxia or tachypnoea it carries OR e^(1.87)=6.46.
```{r}
## Only the commonest items are kept
bulkmodel <- glm(formula = `Major Trauma`=='yes' ~ `multi?` +  
    `MVA ejection` + `MVA entrapment` + 
    `MVA fatality at scene` + 
    `Ped vs car` + `Pen neck` + 
    `Pen torso` + 
    `Burns >15%` + `Near Drown` + 
    `Fall >3m` + `Air Comp` + Intubated + 
    `Sev fac inj` + 
    `HR>120` + `SBP<90` + `Pel Unstab` + 
    `RR>30` + `SaO2<90` + 
    `Resp distr` + Flail + `GCS ≤13` + Agitated + 
    Seizure + `Sig Inj ≥2` , 
    family = "binomial", data = traumata)
bulkmodel
Anova(bulkmodel, test="LR")
drop1(bulkmodel)
vif(bulkmodel)
bulkmodelcomplete <- glm(formula = `Major Trauma`=='yes' ~ `multi?` +  
    `MVA ejection` + `MVA entrapment` + 
    `MVA fatality at scene` + 
    `Ped vs car` + `Pen neck` + 
    `Pen torso` + 
    `Burns >15%` + `Near Drown` + 
    `Fall >3m` + `Air Comp` + Intubated + 
    `Sev fac inj` + 
    `HR>120` + `SBP<90` + `Pel Unstab` + 
    `RR>30` + `SaO2<90` + 
    `Resp distr` + Flail + `GCS ≤13` + Agitated + 
    Seizure + `Sig Inj ≥2` , 
                  family = "binomial", data = traumacompletecrit)
lrtest(wholemodelcomplete, bulkmodelcomplete)
```
The performance of the models in predicting the outcome "Major Trauma" is compared using the likelihood ratio test.  The Likelihood is the probability of the data under a hypothesis.  In all of the calculations below that is the null hypothesis that the odds ratio is 1.
There are limitations to these comparisons.  Firstly, every study like this is subject to sampling error even after excluding the sparse data and some of the fitted probabilities are 1 or 0: perfect prediction. These predictors are not used because they're not true.  It's particularly annoying that 240 observations are deleted due to missingness of one or more variable even after dropping the sparse cells.

Secondly, all else is not equal.  People don't have a middling version of CPR, and the contribution of 2 long bone fractures in the presence of CPR is not independent, it's very much less important than in the absence of CPR.  Or more important, who knows?  The point is that all models have limitations imposed by the size of the dataset used in deriving the model.

Thirdly there are some obviously contributory ones that are excluded from the model, such as CPR and capillary refill time.  A further model needs to be built, the "clinical preference model" where we go down the list and get some colleagues to say think are most useful, and which are least useful.  Then we keep the important ones in and drop the rest one by one.

```{r}
## Remove those with VIF>2:  `Sig Inj ≥2` + `Pel Unstab` + `Sev fac inj` +  Intubated + `Ped vs car`
bulkmodelminimised <- glm(formula = `Major Trauma`=='yes' ~ `multi?` + 
                    `MVA ejection` + `MVA entrapment` + 
                    `MVA fatality at scene` + 
                    `Pen neck` + 
                    `Pen torso` + 
                    `Burns >15%` + `Near Drown` + 
                    `Fall >3m` + `Air Comp` +
                    `HR>120` + `SBP<90` +
                    `RR>30` + `SaO2<90` + 
                    `Resp distr` + Flail + `GCS ≤13` + Agitated + 
                    Seizure, 
                  family = "binomial", data = traumata)
bulkmodelminimised
anova(bulkmodelminimised, test="Chisq")
lrtest(bulkmodel, bulkmodelminimised)
lrtest(wholemodel, bulkmodelminimised)
vif(bulkmodelminimised)
```
Now with the minimised model a couple of expected things happen: the log likelihood is lower by 22.  This is a big difference.  So the p value is 4x10^-8.  Fair enough, the more elaborate model predicts outcomes more closely. But the Variance Inflation Factor has gone up only very slightly for all the remaining factors because the absolute difference between a likelihood of e^(-184) and a likelihood of e^(-162) is not very much, and there are fewer places for unmeasured variance to hide.  These are good things for a model.
```{r}
#Close off data manipulations and save current databases
write_csv(traumata, paste("traumata", "Sys.Date()",".csv"))
write_csv(traumacompletecrit, "traumacompletecriteria.csv")
write_csv(Backgroundrate, "Backgroundrate.csv")
```